# -*- coding: utf-8 -*-
"""Kmedod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1shLBnNZsUZ070Pvvp4N1dIpd6BrzDwX_
"""

!pip install -q pyclustering

from google.colab import drive
drive.mount('/content/drive')

"""Achar os k otimos"""

import numpy as np
import pandas as pd
import os

caminho_csv = "/content/drive/MyDrive/projeto1/resultados/matriz_distancias.csv"

matriz = pd.read_csv(caminho_csv, index_col=0)
dist_matrix = matriz.values

np.fill_diagonal(dist_matrix, 0)

print(f"Matriz carregada com dimens√£o: {dist_matrix.shape}")

RESULTADOS_PATH = "/content/drive/MyDrive/projeto1/resultados"
os.makedirs(RESULTADOS_PATH, exist_ok=True)

from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.spatial.distance import pdist

def dunn_index(data, labels):
    unique_clusters = np.unique(labels)
    min_inter = np.inf
    max_intra = 0
    for i in unique_clusters:
        cluster_i = data[labels == i]
        if cluster_i.shape[0] <= 1:
            continue
        dists = pdist(cluster_i)
        if len(dists) > 0:
            max_intra = max(max_intra, np.max(dists))
        for j in unique_clusters:
            if i >= j:
                continue
            cluster_j = data[labels == j]
            dist_ij = np.min(np.linalg.norm(cluster_i[:, None] - cluster_j[None, :], axis=2))
            if dist_ij < min_inter:
                min_inter = dist_ij
    return min_inter / max_intra if max_intra > 0 else 0

n = dist_matrix.shape[0]
results = []

for k in range(2, 11):
    print(f"Rodando K-Medoids para k={k} ...")
    initial_medoids = np.random.choice(n, k, replace=False).tolist()
    kmedoids_instance = kmedoids(dist_matrix, initial_medoids, data_type='distance_matrix')
    kmedoids_instance.process()
    clusters = kmedoids_instance.get_clusters()


    labels = np.zeros(n, dtype=int)
    for idx, cluster in enumerate(clusters):
        labels[cluster] = idx

    silhouette = silhouette_score(dist_matrix, labels, metric='precomputed')
    try:
        davies_bouldin = davies_bouldin_score(dist_matrix, labels)
    except:
        davies_bouldin = np.nan
    try:
        calinski_harabasz = calinski_harabasz_score(dist_matrix, labels)
    except:
        calinski_harabasz = np.nan
    dunn = dunn_index(dist_matrix, labels)

    custo_total = 0
    for medoid_idx, cluster in enumerate(clusters):
        medoid = cluster[0]  # pegando o primeiro do cluster como refer√™ncia
        custo_total += dist_matrix[np.ix_(cluster, [medoid])].sum()

    results.append({
        'k': k,
        'Custo_Total': custo_total,
        'Silhouette': silhouette,
        'DaviesBouldin': davies_bouldin,
        'Dunn': dunn,
        'CalinskiHarabasz': calinski_harabasz
    })

df_results = pd.DataFrame(results)
df_results.to_csv(os.path.join(RESULTADOS_PATH, "kmedoids_metricas.csv"), index=False)
print("\n‚úÖ M√©tricas salvas em CSV:")
print(df_results)

import pandas as pd
import matplotlib.pyplot as plt

caminho_csv = "/content/drive/MyDrive/projeto1/resultados/kmedoids_metricas.csv"
df = pd.read_csv(caminho_csv)

df_plot = df.copy()
df_plot['Custo_Total_norm'] = df_plot['Custo_Total'] / df_plot['Custo_Total'].max()
df_plot['DaviesBouldin_norm'] = 1 - (df_plot['DaviesBouldin'] / df_plot['DaviesBouldin'].max())
df_plot['CalinskiHarabasz_norm'] = df_plot['CalinskiHarabasz'] / df_plot['CalinskiHarabasz'].max()
df_plot['Dunn_norm'] = df_plot['Dunn'] / df_plot['Dunn'].max()
df_plot['Silhouette_norm'] = df_plot['Silhouette'] / df_plot['Silhouette'].max()

plt.figure(figsize=(12, 7))
plt.plot(df_plot['k'], df_plot['Custo_Total_norm'], marker='o', label='Custo Total (normalizado)')
plt.plot(df_plot['k'], df_plot['Silhouette_norm'], marker='o', label='Silhouette (normalizado)')
plt.plot(df_plot['k'], df_plot['DaviesBouldin_norm'], marker='o', label='Davies-Bouldin (invertido)')
plt.plot(df_plot['k'], df_plot['Dunn_norm'], marker='o', label='Dunn (normalizado)')
plt.plot(df_plot['k'], df_plot['CalinskiHarabasz_norm'], marker='o', label='Calinski-Harabasz (normalizado)')

k_otimo = df_plot.loc[df_plot['Silhouette'].idxmax(), 'k']
plt.axvline(x=k_otimo, color='red', linestyle='--', label=f'k √≥timo ‚âà {k_otimo}')

plt.xlabel("N√∫mero de clusters (k)")
plt.ylabel("M√©tricas normalizadas")
plt.title("Avalia√ß√£o de K-Medoids para diferentes k (curva do cotovelo e m√©tricas)")
plt.grid(True)
plt.legend()
plt.show()

"""Kmedoid"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from pyclustering.cluster.kmedoids import kmedoids
from scipy.spatial.distance import squareform


caminho_csv = "/content/drive/MyDrive/projeto1/resultados/matriz_distancias.csv"
matriz = pd.read_csv(caminho_csv, index_col=0)
dist_matrix = matriz.values
np.fill_diagonal(dist_matrix, 0)

n = dist_matrix.shape[0]
k = 2

initial_medoids = np.random.choice(n, k, replace=False).tolist()
kmedoids_instance = kmedoids(dist_matrix, initial_medoids, data_type='distance_matrix')
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
labels = np.zeros(n, dtype=int)
for idx, cluster in enumerate(clusters):
    labels[cluster] = idx


for i in range(k):
    print(f"Cluster {i+1}: {len(clusters[i])} datasets")


mds2d = MDS(n_components=2, dissimilarity='precomputed', random_state=42)
coords2d = mds2d.fit_transform(dist_matrix)

mds3d = MDS(n_components=3, dissimilarity='precomputed', random_state=42)
coords3d = mds3d.fit_transform(dist_matrix)


colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'brown', 'pink']

plt.figure(figsize=(10,8))
for i in range(k):
    plt.scatter(coords2d[labels==i, 0], coords2d[labels==i, 1], color=colors[i], s=100, label=f'Cluster {i+1}')
    for idx in np.where(labels==i)[0]:
        plt.text(coords2d[idx,0], coords2d[idx,1], matriz.index[idx], fontsize=8)
plt.title("K-Medoids Clusters (2D) - k=2")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.legend()
plt.grid(True)
plt.show()


from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12,10))
ax = fig.add_subplot(111, projection='3d')
for i in range(k):
    ax.scatter(coords3d[labels==i, 0], coords3d[labels==i, 1], coords3d[labels==i, 2],
               color=colors[i], s=100, label=f'Cluster {i+1}')
    for idx in np.where(labels==i)[0]:
        ax.text(coords3d[idx,0], coords3d[idx,1], coords3d[idx,2], matriz.index[idx], fontsize=8)
ax.set_title("K-Medoids Clusters (3D) - k=2")
ax.set_xlabel("Componente 1")
ax.set_ylabel("Componente 2")
ax.set_zlabel("Componente 3")
ax.legend()
plt.show()



df_clusters = pd.DataFrame({
    'Dataset': matriz.index,
    'Cluster': labels + 1  # +1 para ficar Cluster 1, Cluster 2...
})

caminho_saida = "/content/drive/MyDrive/projeto1/resultados/kmedoids_clusters_k2.csv"
df_clusters.to_csv(caminho_saida, index=False)

print(f"Resultados dos clusters salvos em: {caminho_saida}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from pyclustering.cluster.kmedoids import kmedoids
from scipy.spatial.distance import squareform

caminho_csv = "/content/drive/MyDrive/projeto1/resultados/matriz_distancias.csv"
matriz = pd.read_csv(caminho_csv, index_col=0)
dist_matrix = matriz.values
np.fill_diagonal(dist_matrix, 0)
n = dist_matrix.shape[0]
k = 3

initial_medoids = np.random.choice(n, k, replace=False).tolist()
kmedoids_instance = kmedoids(dist_matrix, initial_medoids, data_type='distance_matrix')
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
labels = np.zeros(n, dtype=int)
for idx, cluster in enumerate(clusters):
    labels[cluster] = idx


for i in range(k):
    print(f"Cluster {i+1}: {len(clusters[i])} datasets")


mds2d = MDS(n_components=2, dissimilarity='precomputed', random_state=42)
coords2d = mds2d.fit_transform(dist_matrix)

mds3d = MDS(n_components=3, dissimilarity='precomputed', random_state=42)
coords3d = mds3d.fit_transform(dist_matrix)


colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'brown', 'pink']

plt.figure(figsize=(10,8))
for i in range(k):
    plt.scatter(coords2d[labels==i, 0], coords2d[labels==i, 1], color=colors[i], s=100, label=f'Cluster {i+1}')
    for idx in np.where(labels==i)[0]:
        plt.text(coords2d[idx,0], coords2d[idx,1], matriz.index[idx], fontsize=8)
plt.title("K-Medoids Clusters (2D) - k=2")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.legend()
plt.grid(True)
plt.show()


from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12,10))
ax = fig.add_subplot(111, projection='3d')
for i in range(k):
    ax.scatter(coords3d[labels==i, 0], coords3d[labels==i, 1], coords3d[labels==i, 2],
               color=colors[i], s=100, label=f'Cluster {i+1}')
    for idx in np.where(labels==i)[0]:
        ax.text(coords3d[idx,0], coords3d[idx,1], coords3d[idx,2], matriz.index[idx], fontsize=8)
ax.set_title("K-Medoids Clusters (3D) - k=2")
ax.set_xlabel("Componente 1")
ax.set_ylabel("Componente 2")
ax.set_zlabel("Componente 3")
ax.legend()
plt.show()


df_clusters = pd.DataFrame({
    'Dataset': matriz.index,
    'Cluster': labels + 1  # +1 para ficar Cluster 1, Cluster 2...
})

caminho_saida = "/content/drive/MyDrive/projeto1/resultados/kmedoids_clusters_k3.csv"
df_clusters.to_csv(caminho_saida, index=False)

print(f"Resultados dos clusters salvos em: {caminho_saida}")

"""HDBSCAN"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import hdbscan
from sklearn.decomposition import PCA


caminho_csv = "/content/drive/MyDrive/projeto1/resultados/matriz_distancias.csv"
dist_matrix = pd.read_csv(caminho_csv, index_col=0).values
dataset_names = pd.read_csv(caminho_csv, index_col=0).index


clusterer = hdbscan.HDBSCAN(
    min_cluster_size=10,
    min_samples=5,
    metric='precomputed'
)
labels = clusterer.fit_predict(dist_matrix)

df_clusters = pd.DataFrame({
    'Dataset': dataset_names,
    'Cluster': labels
})

df_clusters['Cluster_size'] = df_clusters.groupby('Cluster')['Cluster'].transform('count')
df_clusters.sort_values(['Cluster', 'Dataset'], inplace=True)
df_clusters.to_csv("/content/drive/MyDrive/projeto1/resultados/hdbscan_clusters.csv", index=False)


pca = PCA(n_components=3)
coords = pca.fit_transform(dist_matrix)


plt.figure(figsize=(10,8))
unique_labels = np.unique(labels)
colors = plt.cm.get_cmap('tab20', len(unique_labels))

for i, cluster in enumerate(unique_labels):
    cluster_points = coords[labels == cluster]
    plt.scatter(cluster_points[:,0], cluster_points[:,1],
                s=50, color=colors(i), label=f"Cluster {cluster}" if cluster != -1 else "Ru√≠do")

    for idx, name in zip(np.where(labels==cluster)[0], dataset_names[labels==cluster]):
        if cluster != -1:
            plt.text(coords[idx,0], coords[idx,1], name, fontsize=8)

plt.title("HDBSCAN - Clusters 2D")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(True)
plt.show()


fig = plt.figure(figsize=(12,10))
ax = fig.add_subplot(111, projection='3d')

for i, cluster in enumerate(unique_labels):
    cluster_points = coords[labels == cluster]
    ax.scatter(cluster_points[:,0], cluster_points[:,1], cluster_points[:,2],
               s=50, color=colors(i), label=f"Cluster {cluster}" if cluster != -1 else "Ru√≠do")
    for idx, name in zip(np.where(labels==cluster)[0], dataset_names[labels==cluster]):
        if cluster != -1:
            ax.text(coords[idx,0], coords[idx,1], coords[idx,2], name, fontsize=8)

ax.set_title("HDBSCAN - Clusters 3D")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
ax.set_zlabel("PC3")
ax.legend()
plt.show()


print("Resumo de clusters HDBSCAN:")
print(df_clusters.groupby('Cluster').size())

"""Represetantes"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA


clusters_path = "/content/drive/MyDrive/projeto1/resultados/kmedoids_clusters_k3.csv"
df_clusters = pd.read_csv(clusters_path, index_col=0)  # coluna 0 = nome do dataset
dataset_names = df_clusters.index.tolist()
labels = df_clusters['Cluster'].values  # coluna 'Cluster' cont√©m os n√∫meros do cluster


representantes = []

for cluster_id in np.unique(labels):
    indices_cluster = np.where(labels == cluster_id)[0]
    cluster_vectors = np.array([dist_matrix[i] for i in indices_cluster])
    mean_vector = cluster_vectors.mean(axis=0)
    dist_to_mean = np.linalg.norm(cluster_vectors - mean_vector, axis=1)
    medoid_idx = indices_cluster[np.argmin(dist_to_mean)]
    medoid_name = dataset_names[medoid_idx]

    representantes.append({
        "Cluster": cluster_id,
        "Medoid_Index": medoid_idx,
        "Dataset_Name": medoid_name
    })

# -------------------------------
# Salvar representantes em CSV
# -------------------------------
df_representantes = pd.DataFrame(representantes)
csv_path = "/content/drive/MyDrive/projeto1/resultados/medoids_representantes_k3.csv"
df_representantes.to_csv(csv_path, index=False)
print(f"‚úÖ Representantes salvos em: {csv_path}")

# -------------------------------
# PCA 2D
# -------------------------------
pca_2D = PCA(n_components=2)
X_2D = pca_2D.fit_transform(dist_matrix)

plt.figure(figsize=(10, 8))
colors = ['r', 'g', 'b', 'c', 'm', 'y']

for cluster_id in np.unique(labels):
    indices_cluster = np.where(labels == cluster_id)[0]
    plt.scatter(X_2D[indices_cluster, 0], X_2D[indices_cluster, 1],
                label=f"Cluster {cluster_id+1}", alpha=0.6)

# Destacar representantes
for rep in representantes:
    idx = rep["Medoid_Index"]
    plt.scatter(X_2D[idx, 0], X_2D[idx, 1], color='k', s=150, marker='X')
    plt.text(X_2D[idx, 0]+0.02, X_2D[idx, 1]+0.02, rep["Dataset_Name"], fontsize=9)

plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("Representantes dos Clusters (K-Medoids, k=3) - 2D")
plt.legend()
plt.grid(True)
plt.show()

# -------------------------------
# PCA 3D
# -------------------------------
pca_3D = PCA(n_components=3)
X_3D = pca_3D.fit_transform(dist_matrix)

fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

for cluster_id in np.unique(labels):
    indices_cluster = np.where(labels == cluster_id)[0]
    ax.scatter(X_3D[indices_cluster, 0], X_3D[indices_cluster, 1], X_3D[indices_cluster, 2],
               color=colors[cluster_id % len(colors)], label=f"Cluster {cluster_id+1}", alpha=0.6)

# Destacar representantes
for rep in representantes:
    idx = rep["Medoid_Index"]
    ax.scatter(X_3D[idx, 0], X_3D[idx, 1], X_3D[idx, 2], color='k', s=150, marker='X')
    ax.text(X_3D[idx, 0]+0.02, X_3D[idx, 1]+0.02, X_3D[idx, 2]+0.02, rep["Dataset_Name"], fontsize=9)

ax.set_xlabel("PCA 1")
ax.set_ylabel("PCA 2")
ax.set_zlabel("PCA 3")
ax.set_title("Representantes dos Clusters (K-Medoids, k=3) - 3D")
ax.legend()
plt.show()

!pip install deap

"""Otimiza√ß√£o dos Hiperpar√¢metros do SVM com NSGA-II"""

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from deap import base, creator, tools, algorithms
import random
import warnings

warnings.filterwarnings("ignore")


rep_path = "/content/drive/MyDrive/projeto1/resultados/medoids_representantes_k3.csv"
df_rep = pd.read_csv(rep_path)
dataset_names = df_rep['Dataset_Name'].tolist()


# Espa√ßo de hiperpar√¢metros SVM em log2
log2_C_range = (-15, 15)
log2_gamma_range = (-15, 15)


# Fun√ß√£o de avalia√ß√£o NSGA-II (Classifica√ß√£o)
def evaluate(params, X, y):

    log2_C, log2_gamma = params
    C = 2 ** log2_C
    gamma = 2 ** log2_gamma

    model = SVC(kernel='rbf', C=C, gamma=gamma)

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    acc_scores, bac_scores, f1_scores = [], [], []

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        acc_scores.append(accuracy_score(y_test, y_pred))
        bac_scores.append(balanced_accuracy_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))


    return -np.mean(acc_scores), -np.mean(bac_scores), -np.mean(f1_scores)


# Configura√ß√£o DEAP
creator.create("FitnessMulti", base.Fitness, weights=(-1.0, -1.0, -1.0))
creator.create("Individual", list, fitness=creator.FitnessMulti)

toolbox = base.Toolbox()
toolbox.register("attr_float", random.uniform, -15, 15)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=2)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=3, indpb=0.3)
toolbox.register("select", tools.selNSGA2)


results = []

for dataset_name in dataset_names:

    print(f"\nüîµ Rodando NSGA-II para dataset: {dataset_name}")


    df = pd.read_csv(f"/content/drive/MyDrive/projeto1/datasets/{dataset_name}.csv")


    target_col = df.columns[-1]
    X = df.drop(columns=[target_col]).values
    y = df[target_col].values


    toolbox.register("evaluate", evaluate, X=X, y=y)


    pop = toolbox.population(n=20)

    # Executar NSGA-II
    algorithms.eaMuPlusLambda(
        pop, toolbox,
        mu=20, lambda_=40,
        cxpb=0.6, mutpb=0.3,
        ngen=10, verbose=False
    )

    best_ind = tools.selBest(pop, 1)[0]
    C_best = 2 ** best_ind[0]
    gamma_best = 2 ** best_ind[1]
    metrics_best = evaluate(best_ind, X, y)

    print(f"‚û° Melhor indiv√≠duo:")
    print(f"   C = {C_best:.6f}")
    print(f"   gamma = {gamma_best:.6f}")
    print(f"   Accuracy = {-metrics_best[0]:.4f}")
    print(f"   BAC = {-metrics_best[1]:.4f}")
    print(f"   F1 = {-metrics_best[2]:.4f}")

    results.append({
        "Dataset": dataset_name,
        "Best_C": C_best,
        "Best_Gamma": gamma_best,
        "Accuracy": -metrics_best[0],
        "Balanced_Accuracy": -metrics_best[1],
        "F1_score": -metrics_best[2]
    })


df_results = pd.DataFrame(results)
df_results.to_csv("/content/drive/MyDrive/projeto1/resultados/best_svm_params_classification.csv", index=False)

print("\n‚úÖ Finalizado! Par√¢metros √≥timos salvos com sucesso!")

"""OS datasets no plano"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/best_svm_params_classification.csv")


df["log2_C"] = np.log2(df["Best_C"])
df["log2_gamma"] = np.log2(df["Best_Gamma"])


plt.figure(figsize=(8,6))
plt.scatter(df["log2_C"], df["log2_gamma"], s=200)


for i, row in df.iterrows():
    plt.text(row["log2_C"] + 0.2, row["log2_gamma"] + 0.2, row["Dataset"], fontsize=12)

plt.xlabel("log‚ÇÇ(C)", fontsize=14)
plt.ylabel("log‚ÇÇ(gamma)", fontsize=14)
plt.title("Posi√ß√£o dos Datasets no Espa√ßo de Hiperpar√¢metros do SVM (NSGA-II)", fontsize=15)
plt.grid(True)
plt.show()

""" MAtriz de dist√¢ncias"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


df = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/best_svm_params_classification.csv")


# Transformar em escala log2 para C e gamma
df["log2_C"] = np.log2(df["Best_C"])
df["log2_gamma"] = np.log2(df["Best_Gamma"])


# Codificar kernel (RBF = 1)
df["kernel_code"] = 1  # s√≥ RBF aqui, se houver outros, colocar c√≥digos diferentes


vectors = df[["log2_C", "log2_gamma", "kernel_code"]].values


def custom_distance(v1, v2, w_C=1.0, w_gamma=1.0, w_kernel=1.0):
    d_C = w_C * abs(v1[0] - v2[0])
    d_gamma = w_gamma * abs(v1[1] - v2[1])
    d_kernel = w_kernel * abs(v1[2] - v2[2])
    return d_C + d_gamma + d_kernel


n = len(vectors)
dist_matrix = np.zeros((n, n))

for i in range(n):
    for j in range(n):
        dist_matrix[i, j] = custom_distance(vectors[i], vectors[j])


dist_df = pd.DataFrame(dist_matrix, index=df["Dataset"], columns=df["Dataset"])

plt.figure(figsize=(8,6))
sns.heatmap(dist_df, annot=True, cmap="viridis")
plt.title("Matriz de Dist√¢ncias entre Datasets (C, gamma, kernel)")
plt.show()

"""Validacao"""

import os
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer


path_medoids = "/content/drive/MyDrive/projeto1/resultados/medoids_representantes_k3.csv"
path_params  = "/content/drive/MyDrive/projeto1/resultados/best_svm_params_classification.csv"
path_newdata = "/content/drive/MyDrive/projeto1/datasets_test"
path_rep_datasets = "/content/drive/MyDrive/projeto1/datasets_representantes"

df_medoids = pd.read_csv(path_medoids)
df_params  = pd.read_csv(path_params)


if "Dataset" not in df_params.columns:
    if "Dataset_Name" in df_params.columns:
        df_params = df_params.rename(columns={"Dataset_Name": "Dataset"})
    else:
        raise ValueError(f"Erro: n√£o encontrei coluna Dataset no arquivo de par√¢metros. Colunas dispon√≠veis: {df_params.columns}")


df_rep = df_medoids.merge(
    df_params,
    left_on="Dataset_Name",
    right_on="Dataset",
    how="left"
)

print("‚úÖ Representantes + hiperpar√¢metros carregados:")
display(df_rep.head())



def dataset_distance(df1, df2):
    stats1 = df1.describe().values.flatten()
    stats2 = df2.describe().values.flatten()
    m = min(len(stats1), len(stats2))
    stats1 = stats1[:m]
    stats2 = stats2[:m]
    return np.linalg.norm(stats1 - stats2)



new_datasets = {}
for file in os.listdir(path_newdata):
    if file.endswith(".csv"):
        name = file.replace(".csv", "")
        try:
            df = pd.read_csv(os.path.join(path_newdata, file))
            new_datasets[name] = df
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar {file}: {e}")

print(f"\n‚úÖ {len(new_datasets)} novos datasets carregados.")


resultados = []

for new_name, new_df in new_datasets.items():

    print(f"\nüìå Processando: {new_name}")


    target_candidates = ["target", "class", "label", "y"]
    target_col = None
    for col in new_df.columns:
        if col.lower() in target_candidates:
            target_col = col
            break
    if target_col is None:
        target_col = new_df.columns[-1]

    X_new = new_df.drop(columns=[target_col])
    y_new = new_df[target_col]


    categorical_cols = X_new.select_dtypes(include=['object']).columns
    numeric_cols = X_new.select_dtypes(exclude=['object']).columns

    preprocess = ColumnTransformer(
        transformers=[
            ('num',
             make_pipeline(SimpleImputer(strategy='median'), StandardScaler()),
             numeric_cols),
            ('cat',
             make_pipeline(SimpleImputer(strategy='most_frequent'),
                           OneHotEncoder(handle_unknown='ignore')),
             categorical_cols)
        ]
    )

    X_new_sc = preprocess.fit_transform(X_new)


    min_dist = float("inf")
    best_rep = None

    for _, row in df_rep.iterrows():
        rep_dataset_name = row["Dataset_Name"]
        rep_path = os.path.join(path_rep_datasets, f"{rep_dataset_name}.csv")

        if not os.path.exists(rep_path):
            continue

        try:
            df_rep_data = pd.read_csv(rep_path)
        except:
            continue

        dist = dataset_distance(new_df, df_rep_data)
        if dist < min_dist:
            min_dist = dist
            best_rep = row

    if best_rep is None:
        print("‚ö†Ô∏è Nenhum representante encontrado para este dataset. Pulando...")
        continue

    print(f"   ‚Üí Representante mais pr√≥ximo: {best_rep['Dataset_Name']}")


    if pd.isna(best_rep.get("Best_C")) or pd.isna(best_rep.get("Best_Gamma")):
        print("‚ö†Ô∏è Representante sem hiperpar√¢metros ‚Äî pulando.")
        continue

    best_C = best_rep["Best_C"]
    best_gamma = best_rep["Best_Gamma"]

    print(f"   ‚Üí C recomendado = {best_C:.4f}")
    print(f"   ‚Üí gamma recomendado = {best_gamma:.4f}")

    model = SVC(C=best_C, gamma=best_gamma, kernel="rbf")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    acc = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="accuracy").mean()
    bal = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="balanced_accuracy").mean()
    f1  = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="f1_macro").mean()

    resultados.append({
        "Dataset_Novo": new_name,
        "Representante": best_rep["Dataset_Name"],
        "C_recomendado": best_C,
        "Gamma_recomendado": best_gamma,
        "Accuracy": acc,
        "Balanced_Accuracy": bal,
        "F1_Macro": f1
    })


df_res = pd.DataFrame(resultados)
output_file = "/content/drive/MyDrive/projeto1/resultados/recomendacao_svm_novos_datasets.csv"
df_res.to_csv(output_file, index=False)

print("\n‚úÖ Arquivo salvo em:")
print(output_file)
display(df_res)

"""Desempenho dos novos datasets com hiperpar√¢metros recomendados"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


df_res = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/recomendacao_svm_novos_datasets.csv")

df_plot = df_res.melt(
    id_vars=["Dataset_Novo", "Representante"],
    value_vars=["Accuracy", "Balanced_Accuracy", "F1_Macro"],
    var_name="M√©trica",
    value_name="Valor"
)
representantes = df_plot["Representante"].unique()
palette = sns.color_palette("tab10", len(representantes))
color_map = dict(zip(representantes, palette))

plt.figure(figsize=(16,6))
sns.barplot(
    data=df_plot,
    x="Dataset_Novo",
    y="Valor",
    hue="Representante",
    palette=color_map
)

plt.xticks(rotation=45, ha='right')
plt.ylim(0,1.05)
plt.title("Desempenho dos novos datasets com hiperpar√¢metros recomendados")
plt.ylabel("Valor da M√©trica")
plt.xlabel("Dataset")
plt.legend(title="Representante", bbox_to_anchor=(1.05,1), loc='upper left')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df_res = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/recomendacao_svm_novos_datasets.csv")

metricas = ["Accuracy", "Balanced_Accuracy", "F1_Macro"]

for metrica in metricas:
    plt.figure(figsize=(max(12, len(df_res["Dataset_Novo"].unique())*0.8),
                        max(6, len(df_res["Representante"].unique())*0.6)))

    heat_data = df_res.pivot(index="Representante", columns="Dataset_Novo", values=metrica)

    sns.heatmap(
        heat_data,
        annot=True,
        fmt=".2f",
        cmap="YlGnBu",
        cbar_kws={'label': metrica},
        linewidths=0.5
    )

    plt.title(f"{metrica} por Representante e Dataset", fontsize=14)
    plt.ylabel("Representante", fontsize=12)
    plt.xlabel("Dataset", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

"""Desempenho medio por represetante"""

avg_performance = df_res.groupby("Representante")[["Accuracy","Balanced_Accuracy","F1_Macro"]].mean()
plt.figure(figsize=(8,5))
sns.heatmap(avg_performance, annot=True, fmt=".2f", cmap="YlGnBu", cbar_kws={'label': 'M√©dia da m√©trica'})
plt.title("Desempenho m√©dio por Representante")
plt.ylabel("Representante")
plt.xlabel("M√©trica")
plt.show()

"""Parametros Padrao"""

resultados_padrao = []

for new_name, new_df in new_datasets.items():

    print(f"\nüìå Processando (padr√£o): {new_name}")

    target_candidates = ["target", "class", "label", "y"]
    target_col = None
    for col in new_df.columns:
        if col.lower() in target_candidates:
            target_col = col
            break
    if target_col is None:
        target_col = new_df.columns[-1]

    X_new = new_df.drop(columns=[target_col])
    y_new = new_df[target_col]


    categorical_cols = X_new.select_dtypes(include=['object']).columns
    numeric_cols = X_new.select_dtypes(exclude=['object']).columns

    preprocess = ColumnTransformer(
        transformers=[
            ('num',
             make_pipeline(SimpleImputer(strategy='median'), StandardScaler()),
             numeric_cols),
            ('cat',
             make_pipeline(SimpleImputer(strategy='most_frequent'),
                           OneHotEncoder(handle_unknown='ignore')),
             categorical_cols)
        ]
    )

    X_new_sc = preprocess.fit_transform(X_new)


    model = SVC(C=1.0, gamma='scale', kernel='rbf')
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    acc = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="accuracy").mean()
    bal = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="balanced_accuracy").mean()
    f1  = cross_val_score(model, X_new_sc, y_new, cv=cv, scoring="f1_macro").mean()

    resultados_padrao.append({
        "Dataset_Novo": new_name,
        "C_padrao": 1.0,
        "Gamma_padrao": 'scale',
        "Accuracy": acc,
        "Balanced_Accuracy": bal,
        "F1_Macro": f1
    })


df_res_padrao = pd.DataFrame(resultados_padrao)
output_file_padrao = "/content/drive/MyDrive/projeto1/resultados/svm_padrao_novos_datasets.csv"
df_res_padrao.to_csv(output_file_padrao, index=False)

print("\n‚úÖ Arquivo SVM padr√£o salvo em:")
print(output_file_padrao)
display(df_res_padrao)

"""Grafico que demostra o desempenho de hiperparametros padrao"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df_padrao = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/svm_padrao_novos_datasets.csv")

plt.figure(figsize=(14, 6))

x = np.arange(len(df_padrao))

width = 0.25

plt.bar(x - width, df_padrao["Accuracy"], width, label="Accuracy")
plt.bar(x, df_padrao["Balanced_Accuracy"], width, label="Balanced Accuracy")
plt.bar(x + width, df_padrao["F1_Macro"], width, label="F1 Macro")

plt.xticks(x, df_padrao["Dataset_Novo"], rotation=45, ha="right")
plt.ylabel("Desempenho")
plt.title("Desempenho SVM (hiperpar√¢metros padr√£o) nos novos datasets")
plt.ylim(0, 1.05)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df_padrao = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/svm_padrao_novos_datasets.csv")

metrics = ["Accuracy", "Balanced_Accuracy", "F1_Macro"]
num_metrics = len(metrics)

angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()
angles += angles[:1]

fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

for idx, row in df_padrao.iterrows():
    values = row[metrics].tolist()
    values += values[:1]
    ax.plot(angles, values, label=row["Dataset_Novo"])
    ax.fill(angles, values, alpha=0.1)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics)
ax.set_yticks(np.linspace(0, 1, 6))
ax.set_yticklabels([f"{x:.1f}" for x in np.linspace(0, 1, 6)])
ax.set_title("Desempenho SVM (hiperpar√¢metros padr√£o) nos novos datasets", size=14, y=1.1)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

plt.show()

"""heatmap de desempenho usando os hiperpar√¢metros padr√£o"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# carregar resultados SVM padr√£o
df_res = pd.read_csv("/content/drive/MyDrive/projeto1/resultados/svm_padrao_novos_datasets.csv")

metricas = ["Accuracy", "Balanced_Accuracy", "F1_Macro"]

for metrica in metricas:
    plt.figure(figsize=(12, 6))
    sns.barplot(
        x="Dataset_Novo",
        y=metrica,
        data=df_res,
        palette="YlGnBu"
    )
    plt.title(f"{metrica} dos Novos Datasets (SVM padr√£o)", fontsize=14)
    plt.ylabel(metrica, fontsize=12)
    plt.xlabel("Dataset", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

from google.colab import drive
drive.mount('/content/drive')