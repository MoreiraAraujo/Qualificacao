# -*- coding: utf-8 -*-
"""myeval.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnvwS8zQ-vd_em1zIkLV369u_6N1Svbr
"""

def carregar_datasets(pasta: str) -> dict:
    """
    Carrega todos os CSVs da pasta, mantendo apenas colunas numÃ©ricas.
    Realiza imputaÃ§Ã£o de valores ausentes e normalizaÃ§Ã£o segura.
    Se o dataset tiver apenas 1 coluna, adiciona coluna extra para permitir PCA.

    Args:
        pasta (str): Caminho da pasta com os arquivos CSV.

    Returns:
        dict: DicionÃ¡rio {nome_dataset: DataFrame normalizado}.
    """
    datasets = {}
    arquivos = [f for f in os.listdir(pasta) if f.endswith(".csv")]

    for arq in arquivos:
        caminho = os.path.join(pasta, arq)
        try:
            df = pd.read_csv(caminho)
            df_num = df.select_dtypes(include=[np.number])

            # Remove colunas completamente NaN
            df_num = df_num.dropna(axis=1, how='all')

            # Remove linhas completamente NaN
            df_num = df_num.dropna(axis=0, how='all')

            # Se nÃ£o sobrar nenhuma coluna, cria coluna dummy
            if df_num.shape[1] == 0:
                df_num = pd.DataFrame(np.zeros((df.shape[0], 1)), columns=["col_dummy"])

            # Se tiver apenas 1 coluna, adiciona coluna extra
            if df_num.shape[1] == 1:
                # OpÃ§Ã£o: duplicar a coluna ou adicionar ruÃ­do pequeno
                df_num['col_aux'] = df_num.iloc[:, 0] + np.random.normal(0, 1e-6, size=df_num.shape[0])

            # ImputaÃ§Ã£o com mÃ©dia
            imputer = SimpleImputer(strategy="mean")
            df_filled = pd.DataFrame(imputer.fit_transform(df_num), columns=df_num.columns)

            # NormalizaÃ§Ã£o segura (evita divisÃ£o por zero)
            stds = df_filled.std()
            stds[stds == 0] = 1
            df_scaled = (df_filled - df_filled.mean()) / stds

            if df_scaled.shape[0] > 0:
                datasets[arq.replace(".csv", "")] = df_scaled
            else:
                print(f"âš ï¸ {arq} nÃ£o tem linhas vÃ¡lidas e serÃ¡ ignorado")

        except Exception as e:
            print(f"âš ï¸ Erro ao ler {arq}: {e}")

    return datasets

# myeval.py
# -*- coding: utf-8 -*-

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
import sys
sys.path.append("/content/drive/MyDrive/projeto1")  # caminho onde estÃ¡ o pcametric.py
from pcametric import PCAMetric   # agora funciona

# -------------------------------
# Caminhos
# -------------------------------
BASE_PATH = "/content/drive/MyDrive/projeto1"
DATASETS_PATH = os.path.join(BASE_PATH, "datasets")
RESULTADOS_PATH = os.path.join(BASE_PATH, "resultados")
os.makedirs(RESULTADOS_PATH, exist_ok=True)

# -------------------------------
# FunÃ§Ã£o para carregar datasets
# -------------------------------
def carregar_datasets(pasta: str) -> dict:
    datasets = {}
    arquivos = [f for f in os.listdir(pasta) if f.endswith(".csv")]

    for arq in arquivos:
        caminho = os.path.join(pasta, arq)
        try:
            df = pd.read_csv(caminho)
            df_num = df.select_dtypes(include=[np.number])

            # Remove colunas e linhas totalmente NaN
            df_num = df_num.dropna(axis=1, how='all').dropna(axis=0, how='all')

            # Nenhuma coluna numÃ©rica â†’ criar dummy
            if df_num.shape[1] == 0:
                df_num = pd.DataFrame(np.zeros((df.shape[0], 1)), columns=["col_dummy"])

            # Apenas 1 coluna â†’ adicionar coluna auxiliar com ruÃ­do mÃ­nimo
            if df_num.shape[1] == 1:
                df_num['col_aux'] = df_num.iloc[:, 0] + np.random.normal(0, 1e-6, size=df_num.shape[0])

            # ImputaÃ§Ã£o com mÃ©dia
            imputer = SimpleImputer(strategy="mean")
            df_filled = pd.DataFrame(imputer.fit_transform(df_num), columns=df_num.columns)

            # NormalizaÃ§Ã£o segura
            stds = df_filled.std()
            stds[stds == 0] = 1
            df_scaled = (df_filled - df_filled.mean()) / stds

            if df_scaled.shape[0] > 0:
                datasets[arq.replace(".csv", "")] = df_scaled
            else:
                print(f"âš ï¸ {arq} nÃ£o tem linhas vÃ¡lidas e serÃ¡ ignorado")

        except Exception as e:
            print(f"âš ï¸ Erro ao ler {arq}: {e}")

    return datasets

# -------------------------------
# DistÃ¢ncia simples para casos degenerados
# -------------------------------
def distancia_simples(df1: pd.DataFrame, df2: pd.DataFrame) -> float:
    mean_diff = abs(df1.mean().mean() - df2.mean().mean())
    std_sum = df1.std().mean() + df2.std().mean()
    if std_sum == 0:
        return mean_diff
    return mean_diff / std_sum

# -------------------------------
# Reduzir datasets para mesma dimensÃ£o
# -------------------------------
def reduzir_para_mesmo_dim(df1: pd.DataFrame, df2: pd.DataFrame):
    min_cols = min(df1.shape[1], df2.shape[1])
    return df1.iloc[:, :min_cols], df2.iloc[:, :min_cols]

# -------------------------------
# Gerar matriz de distÃ¢ncias usando PCAMetric
# -------------------------------
def gerar_matriz(datasets: dict) -> pd.DataFrame:
    nomes = list(datasets.keys())
    n = len(nomes)
    matriz = np.zeros((n, n))

    for i in range(n):
        for j in range(n):
            df1, df2 = datasets[nomes[i]], datasets[nomes[j]]
            try:
                # DistÃ¢ncia simples para datasets degenerados
                if df1.shape[0] < 2 or df2.shape[0] < 2 or df1.shape[1] < 2 or df2.shape[1] < 2:
                    distancia = distancia_simples(df1, df2)
                else:
                    df1_r, df2_r = reduzir_para_mesmo_dim(df1, df2)
                    n_comp = min(df1_r.shape[0], df2_r.shape[0], df1_r.shape[1], df2_r.shape[1], 50)
                    if n_comp < 2:
                        distancia = distancia_simples(df1, df2)
                    else:
                        res, _, _ = PCAMetric(df1_r, df2_r, num_components=n_comp, normalization="precise", preprocess="std")
                        distancia = (res["exp_var_diff"] + res["comp_angle_diff"]) / 2

                matriz[i, j] = matriz[j, i] = distancia

            except Exception as e:
                print(f"âŒ Erro comparando {nomes[i]} x {nomes[j]}: {e}")
                matriz[i, j] = matriz[j, i] = np.nan

    return pd.DataFrame(matriz, index=nomes, columns=nomes)

# -------------------------------
# ExecuÃ§Ã£o principal
# -------------------------------
if __name__ == "__main__":
    print("ğŸ”¹ Carregando datasets...")
    datasets = carregar_datasets(DATASETS_PATH)
    print(f"Total carregado: {len(datasets)}")

    if len(datasets) == 0:
        raise RuntimeError("âŒ Nenhum dataset carregado!")

    print("ğŸ”¹ Calculando matriz de distÃ¢ncia...")
    matriz = gerar_matriz(datasets)

    out = os.path.join(RESULTADOS_PATH, "matriz_distancias.csv")
    matriz.to_csv(out)
    print(f"âœ… Matriz de distÃ¢ncias salva em: {out}")

    # -------------------------------
    # Plotar heatmap
    # -------------------------------
    plt.figure(figsize=(20, 18))
    sns.set(font_scale=0.8)
    sns.heatmap(matriz, cmap="viridis", linewidths=0.5, linecolor='white', square=True,
                cbar_kws={'label': 'DistÃ¢ncia'})
    plt.title("Heatmap de DistÃ¢ncias entre Datasets", fontsize=18)
    plt.tight_layout()
    heatmap_path = os.path.join(RESULTADOS_PATH, "heatmap_distancias.png")
    plt.savefig(heatmap_path, dpi=300)
    plt.show()
    print(f"âœ… Heatmap salvo em: {heatmap_path}")

# dendrogramas.py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform

# -------------------------------
# Caminhos
# -------------------------------
BASE_PATH = "/content/drive/MyDrive/projeto1"
RESULTADOS_PATH = os.path.join(BASE_PATH, "resultados")
MATRIZ_PATH = os.path.join(RESULTADOS_PATH, "matriz_distancias.csv")
os.makedirs(RESULTADOS_PATH, exist_ok=True)

# -------------------------------
# Carregar matriz de distÃ¢ncias
# -------------------------------
matriz = pd.read_csv(MATRIZ_PATH, index_col=0)
nomes = matriz.index.tolist()

# -------------------------------
# Corrigir a diagonal (distÃ¢ncia de cada dataset para ele mesmo = 0)
# -------------------------------
np.fill_diagonal(matriz.values, 0)

# -------------------------------
# Criar vetor condensado (necessÃ¡rio para linkage)
# -------------------------------
dist_vect = squareform(matriz.values)

# -------------------------------
# Tipos de linkage
# -------------------------------
linkages = ["single", "complete", "average", "ward", "centroid"]

for link in linkages:
    # Ward sÃ³ funciona com mÃ©tricas euclidianas
    if link == "ward":
        # Linkage com Ward exige matriz de observaÃ§Ãµes, nÃ£o de distÃ¢ncias
        print(f"âš ï¸ Linkage '{link}' precisa de matriz de observaÃ§Ãµes. Pulando.")
        continue

    Z = linkage(dist_vect, method=link)

    plt.figure(figsize=(16, 10))
    dendrogram(Z, labels=nomes, leaf_rotation=90, leaf_font_size=10)
    plt.title(f"Dendrograma - Linkage: {link}", fontsize=16)
    plt.tight_layout()

    # Salvar figura
    out_path = os.path.join(RESULTADOS_PATH, f"dendrogram_{link}.png")
    plt.savefig(out_path, dpi=300)
    plt.show()
    print(f"âœ… Dendrograma '{link}' salvo em: {out_path}")

# dendrogramas_grid.py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform

# -------------------------------
# Caminhos
# -------------------------------
BASE_PATH = "/content/drive/MyDrive/projeto1"
RESULTADOS_PATH = os.path.join(BASE_PATH, "resultados")
MATRIZ_PATH = os.path.join(RESULTADOS_PATH, "matriz_distancias.csv")
os.makedirs(RESULTADOS_PATH, exist_ok=True)

# -------------------------------
# Carregar matriz de distÃ¢ncias
# -------------------------------
matriz = pd.read_csv(MATRIZ_PATH, index_col=0)
nomes = matriz.index.tolist()

# -------------------------------
# Corrigir a diagonal
# -------------------------------
np.fill_diagonal(matriz.values, 0)

# -------------------------------
# Vetor condensado
# -------------------------------
dist_vect = squareform(matriz.values)

# -------------------------------
# Tipos de linkage
# -------------------------------
linkages = ["single", "complete", "average", "ward", "centroid"]

# Criar figura com 3x2 subplots (uma cÃ©lula ficarÃ¡ vazia)
fig, axes = plt.subplots(3, 2, figsize=(20, 18))
axes = axes.flatten()

for idx, link in enumerate(linkages):
    ax = axes[idx]

    if link == "ward":
        print(f"âš ï¸ Linkage '{link}' precisa de matriz de observaÃ§Ãµes. Pulando.")
        ax.axis('off')
        ax.set_title(f"{link} (nÃ£o aplicÃ¡vel)")
        continue

    Z = linkage(dist_vect, method=link)
    dendrogram(Z, labels=nomes, leaf_rotation=90, leaf_font_size=10, ax=ax)
    ax.set_title(f"{link} Linkage", fontsize=14)

# Desligar qualquer subplot sobrando
for j in range(len(linkages), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
out_path = os.path.join(RESULTADOS_PATH, "dendrogramas_grid.png")
plt.savefig(out_path, dpi=300)
plt.show()
print(f"âœ… Dendrogramas salvos em grade: {out_path}")

"""Dendogramas"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform

BASE_PATH = "/content/drive/MyDrive/projeto1"
RESULTADOS_PATH = os.path.join(BASE_PATH, "resultados")
MATRIZ_PATH = os.path.join(RESULTADOS_PATH, "matriz_distancias.csv")
os.makedirs(RESULTADOS_PATH, exist_ok=True)

matriz = pd.read_csv(MATRIZ_PATH, index_col=0)
nomes = matriz.index.tolist()

np.fill_diagonal(matriz.values, 0)

dist_vect = squareform(matriz.values)

"""Single Linkage"""

Z_single = linkage(dist_vect, method='single')
plt.figure(figsize=(12, 6))
dendrogram(Z_single, labels=nomes, leaf_rotation=90, leaf_font_size=10)
plt.title("Single Linkage")
plt.tight_layout()
plt.show()

plt.savefig(os.path.join(RESULTADOS_PATH, "dendrogram_single.png"), dpi=300)
plt.show()

"""Complete Linkage"""

Z_complete = linkage(dist_vect, method='complete')
plt.figure(figsize=(12, 6))
dendrogram(Z_complete, labels=nomes, leaf_rotation=90, leaf_font_size=10)
plt.title("Complete Linkage")
plt.tight_layout()
plt.show()

plt.savefig(os.path.join(RESULTADOS_PATH, "dendrogram_complete.png"), dpi=300)
plt.show()

"""Average Linkage"""

Z_average = linkage(dist_vect, method='average')
plt.figure(figsize=(12, 6))
dendrogram(Z_average, labels=nomes, leaf_rotation=90, leaf_font_size=10)
plt.title("Average Linkage")
plt.tight_layout()
plt.show()

plt.savefig(os.path.join(RESULTADOS_PATH, "dendrogram_average.png"), dpi=300)
plt.show()

"""Ward Linkage"""

from sklearn.manifold import MDS

mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)
coords = mds.fit_transform(matriz.values)

Z_ward = linkage(coords, method='ward')
plt.figure(figsize=(12, 6))
dendrogram(Z_ward, labels=nomes, leaf_rotation=90, leaf_font_size=10)
plt.title("Ward Linkage (via MDS)")
plt.tight_layout()
plt.show()

plt.savefig(os.path.join(RESULTADOS_PATH, "dendrogram_ward.png"), dpi=300)
plt.show()

"""Centroid Linkage"""

Z_centroid = linkage(dist_vect, method='centroid')
plt.figure(figsize=(12, 6))
dendrogram(Z_centroid, labels=nomes, leaf_rotation=90, leaf_font_size=10)
plt.title("Centroid Linkage")
plt.tight_layout()
plt.show()

plt.savefig(os.path.join(RESULTADOS_PATH, "dendrogram_centroid.png"), dpi=300)
plt.show()

"""Distribuicao das bases no plano"""

mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)
coords = mds.fit_transform(matriz.values)

# coords[:,0] e coords[:,1] sÃ£o os eixos x e y
plt.figure(figsize=(12, 10))
plt.scatter(coords[:,0], coords[:,1], s=100, c='skyblue', edgecolor='k')

# Adicionar labels
for i, nome in enumerate(nomes):
    plt.text(coords[i,0]+0.01, coords[i,1]+0.01, nome, fontsize=9)

plt.title("DistribuiÃ§Ã£o dos datasets no plano (MDS 2D)", fontsize=16)
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.tight_layout()

# Salvar figura
plt.savefig(os.path.join(RESULTADOS_PATH, "datasets_2D_MDS.png"), dpi=300)
plt.show()